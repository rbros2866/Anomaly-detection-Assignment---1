{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection is a technique used in data analysis to identify patterns or observations that deviate significantly from the norm or expected behavior within a dataset. The purpose of anomaly detection is to pinpoint unusual or suspicious instances that may indicate errors, outliers, or potentially interesting phenomena.\n",
    "\n",
    "**Here are some key points about anomaly detection:**\n",
    "\n",
    "**Identification of Deviations:** Anomaly detection involves identifying data points, events, or observations that do not conform to the expected behavior or patterns within a dataset.\n",
    "\n",
    "**Various Applications:** It is employed across various domains such as cybersecurity, fraud detection, network monitoring, system health monitoring, industrial quality control, and financial transaction monitoring.\n",
    "\n",
    "**Methods and Techniques:** There are numerous methods and techniques for anomaly detection, including statistical approaches, machine learning algorithms (such as clustering, classification, and density estimation), and domain-specific rule-based methods.\n",
    "\n",
    "**Unsupervised vs. Supervised Learning:** Anomaly detection can be performed using both supervised and unsupervised learning approaches. Unsupervised methods are commonly used when labeled data is scarce or unavailable, whereas supervised methods require labeled data to train the model.\n",
    "\n",
    "**Real-time Detection:** In many applications, real-time anomaly detection is crucial to quickly respond to unusual events or behaviors, such as in network intrusion detection systems or monitoring equipment in industrial processes.\n",
    "\n",
    "**Challenges:** Anomaly detection faces challenges such as defining what constitutes normal behavior, dealing with imbalanced datasets where anomalies are rare, and ensuring that detected anomalies are truly meaningful and not false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Normal Behavior:** One of the fundamental challenges in anomaly detection is defining what constitutes normal behavior within a dataset. This requires a deep understanding of the domain and the data being analyzed. In many cases, normal behavior may be subjective or change over time, making it challenging to establish a definitive baseline.\n",
    "\n",
    "**Imbalanced Datasets:** Anomaly detection often deals with imbalanced datasets where normal instances significantly outnumber anomalous instances. This can lead to difficulties in training models that are biased towards normal instances and may struggle to detect rare anomalies effectively.\n",
    "\n",
    "**Labeling Anomalies:** In supervised anomaly detection, labeling anomalies in the training dataset can be a labor-intensive and subjective process. Anomalies may also evolve over time, requiring constant reevaluation and updating of the training dataset.\n",
    "\n",
    "**Adaptation to Changing Environments:** Anomalies may arise due to changes in the underlying data distribution or environment. Anomaly detection models need to be able to adapt to these changes and continuously learn from new data to maintain their effectiveness.\n",
    "\n",
    "**Noise and Outliers:** Distinguishing between anomalies and noise or outliers can be challenging, especially in datasets with high levels of variability or measurement errors. It's essential to develop robust methods that can differentiate between meaningful anomalies and insignificant fluctuations.\n",
    "\n",
    "**Scalability:** Anomaly detection algorithms need to be scalable to handle large volumes of data efficiently, particularly in real-time applications where timely detection is critical. Scalability considerations include computational resources, memory requirements, and algorithmic efficiency.\n",
    "\n",
    "**Interpretability:** While some anomaly detection techniques produce binary outcomes (anomaly or not), others provide anomaly scores or rankings, which may be challenging to interpret. Ensuring the interpretability of anomaly detection results is crucial for effective decision-making and action.\n",
    "\n",
    "**False Positives and False Negatives:** Balancing the trade-off between false positives (normal instances incorrectly flagged as anomalies) and false negatives (anomalies missed by the detection algorithm) is essential. Minimizing both types of errors requires careful tuning of model parameters and evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised Anomaly Detection:**\n",
    "\n",
    "**Definition:** In unsupervised anomaly detection, the algorithm aims to identify anomalies in a dataset without the use of labeled data. The algorithm searches for instances that deviate significantly from the majority of the data, assuming that anomalies are rare and different from the normal behavior observed in the dataset.\n",
    "\n",
    "**Methodology:** Unsupervised anomaly detection techniques include clustering-based methods, density estimation, nearest neighbor-based approaches, and statistical methods like Gaussian distribution modeling or isolation forests.\n",
    "\n",
    "**Usage:** Unsupervised anomaly detection is useful when labeled data is scarce or unavailable, making it suitable for exploratory analysis or situations where anomalies may not be well-defined in advance. It is often used for outlier detection, novelty detection, or anomaly discovery in various domains.\n",
    "\n",
    "**Supervised Anomaly Detection:**\n",
    "\n",
    "**Definition:** Supervised anomaly detection involves training a model using labeled data, where both normal and anomalous instances are explicitly identified. The model learns to distinguish between normal and anomalous behavior based on the labeled examples provided during training.\n",
    "\n",
    "**Methodology:** Supervised anomaly detection typically involves using classification algorithms, such as support vector machines (SVM), decision trees, random forests, or neural networks. The model is trained on labeled data, with the objective of maximizing classification accuracy.\n",
    "\n",
    "**Usage:** Supervised anomaly detection is suitable when labeled data is available and anomalies are well-defined. It is commonly used in applications where the cost of misclassifying anomalies is high, such as fraud detection, intrusion detection, or medical diagnosis.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "**Label Requirement:** Unsupervised anomaly detection does not require labeled data for training, whereas supervised anomaly detection relies on labeled data to train the model.\n",
    "\n",
    "**Assumption about Anomalies:** Unsupervised methods assume that anomalies are rare and different from normal instances, while supervised methods explicitly learn the characteristics of anomalies from labeled examples.\n",
    "\n",
    "**Flexibility:** Unsupervised methods are more flexible and can adapt to novel types of anomalies without the need for retraining, whereas supervised methods may struggle to detect unseen anomalies not present in the training data.\n",
    "\n",
    "**Applicability:** Unsupervised methods are suitable for exploratory analysis or when labeled data is scarce, while supervised methods are preferable when labeled examples of anomalies are available and the distinction between normal and anomalous instances is well-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical Methods:**\n",
    "\n",
    "Z-Score: This method involves calculating the z-score of each data point based on the mean and standard deviation of the dataset. Points with z-scores exceeding a certain threshold are considered anomalies.\n",
    "\n",
    "Grubbs' Test: Grubbs' test is a statistical test used to detect outliers in a univariate dataset by comparing the largest observed value to the expected range under the assumption of normality.\n",
    "\n",
    "Histogram-based Methods: These methods partition the data into bins and calculate statistics such as frequency or density within each bin. Anomalies are identified as data points falling into sparsely populated bins.\n",
    "\n",
    "**Machine Learning Algorithms:**\n",
    "\n",
    "Clustering: Clustering algorithms like k-means or DBSCAN can be used for anomaly detection by identifying clusters of normal data points and flagging points that fall outside of these clusters as anomalies.\n",
    "\n",
    "Classification: Supervised classification algorithms such as Support Vector Machines (SVM), Decision Trees, Random Forests, or Neural Networks can be trained on labeled data to classify instances as normal or anomalous.\n",
    "\n",
    "Density Estimation: Algorithms like Gaussian Mixture Models (GMM), Kernel Density Estimation (KDE), or One-Class SVM estimate the probability density function of the data and identify anomalies as points with low probability density.\n",
    "\n",
    "**Proximity-based Methods:**\n",
    "\n",
    "Nearest Neighbor: Nearest neighbor methods identify anomalies based on their distance to their nearest neighbors. Instances that are significantly distant from their neighbors are considered anomalies.\n",
    "\n",
    "Local Outlier Factor (LOF): LOF measures the local density deviation of a data point with respect to its neighbors. Anomalies are points with significantly lower density compared to their neighbors.\n",
    "\n",
    "**Information Theory Methods:**\n",
    "\n",
    "Entropy-based Methods: These methods measure the uncertainty or disorder in the dataset and identify anomalies as instances that increase the overall entropy or information content of the data.\n",
    "\n",
    "Kullback-Leibler (KL) Divergence: KL divergence quantifies the difference between two probability distributions. Anomalies are detected by identifying instances that significantly deviate from the expected distribution.\n",
    "\n",
    "**Rule-based Methods:**\n",
    "\n",
    "Expert Knowledge Rules: These methods rely on domain-specific rules or heuristics defined by experts to identify anomalies. Rules may be based on logical conditions, thresholds, or patterns indicative of anomalous behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods rely on the assumption that anomalies in a dataset can be identified based on their unusual distance or dissimilarity from the majority of the data points. These methods make several key assumptions:\n",
    "\n",
    "**Normal Instances Cluster Together:** Distance-based anomaly detection methods assume that normal instances in the dataset tend to cluster together in the feature space. This means that most of the data points representing normal behavior are close to each other, forming dense clusters.\n",
    "\n",
    "**Anomalies are Isolated or Sparse:** Conversely, anomalies are assumed to be isolated or sparse compared to normal instances. That is, anomalies do not conform to the patterns exhibited by normal instances and are often located far away from dense clusters or regions of high data density.\n",
    "\n",
    "**Distance Metric is Meaningful:** These methods assume the existence of a meaningful distance metric or similarity measure that accurately captures the dissimilarity between data points. Common distance metrics include Euclidean distance, Mahalanobis distance, cosine similarity, or Manhattan distance.\n",
    "\n",
    "**Presence of Outliers:** Distance-based methods acknowledge the presence of outliers within the dataset but assume that these outliers are not representative of anomalous behavior. Outliers may arise due to measurement errors, data corruption, or extreme values within the normal range of behavior.\n",
    "\n",
    "**Data Distribution is Known or Approximated:** Some distance-based methods may assume knowledge of the underlying data distribution or attempt to approximate it. For instance, methods like k-nearest neighbors or density-based clustering rely on estimating local data density to identify anomalies.\n",
    "\n",
    "**Homogeneity of Data:** Distance-based methods often assume homogeneity of data, meaning that the characteristics and distribution of the data remain consistent throughout the dataset. Deviations from this homogeneity may indicate potential anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local Density Calculation:**\n",
    "\n",
    "For each data point, the algorithm identifies its k nearest neighbors based on a chosen distance metric like Euclidean distance.\n",
    "\n",
    "These neighbors form a neighborhood around the point.\n",
    "\n",
    "The local density of the point is calculated as the inverse of the average reachability distance of its nearest neighbors. Reachability distance measures how reachable a point is from its neighbors.\n",
    "\n",
    "**Local Reachability Density Calculation:**\n",
    "\n",
    "The local reachability density of a point is computed as the inverse of the average reachability distance of its neighbors.\n",
    "\n",
    "**Local Outlier Factor Calculation:**\n",
    "\n",
    "For each point, the local outlier factor (LOF) is computed by comparing its local density to that of its neighbors.\n",
    "\n",
    "LOF indicates how much more or less dense a point is compared to its neighbors. A high LOF suggests that the point is an outlier.\n",
    "\n",
    "**Anomaly Score:**\n",
    "\n",
    "Anomaly scores are derived from the LOF values. Higher LOF values indicate points that are more likely to be anomalies.\n",
    "\n",
    "Anomaly scores are often normalized to a specific range (e.g., [0, 1]) for easier interpretation and comparison.\n",
    "\n",
    "In simple terms, the LOF algorithm identifies anomalies by comparing the local density of each point to that of its neighbors. Points with significantly lower local densities compared to their neighbors are assigned higher anomaly scores, indicating that they are more likely to be anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Trees (n_estimators):**\n",
    "\n",
    "This parameter specifies the number of trees to be built in the Isolation Forest ensemble. Increasing the number of trees can improve the accuracy of anomaly detection but may also increase computational time.\n",
    "\n",
    "**Subsample Size (max_samples):**\n",
    "\n",
    "The max_samples parameter determines the number of samples to be drawn from the dataset to build each tree in the forest. A smaller subsample size can speed up the training process but may reduce the effectiveness of the algorithm.\n",
    "\n",
    "**Maximum Tree Depth (max_depth):**\n",
    "\n",
    "The max_depth parameter controls the maximum depth of each decision tree in the forest. Limiting the tree depth helps prevent overfitting to the training data and improves the generalization ability of the model.\n",
    "\n",
    "**Contamination:**\n",
    "\n",
    "The contamination parameter specifies the expected proportion of anomalies in the dataset. This parameter helps adjust the decision threshold for classifying instances as anomalies. Higher contamination values result in a more aggressive detection of anomalies but may also increase the risk of false positives.\n",
    "\n",
    "**Bootstrap Sampling (bootstrap):**\n",
    "\n",
    "The bootstrap parameter determines whether bootstrap sampling is used to select samples for training each decision tree. Setting bootstrap to True enables bootstrap sampling, while setting it to False disables it. Bootstrap sampling can introduce diversity into the ensemble and improve the robustness of the model.\n",
    "\n",
    "**Random Seed (random_state):**\n",
    "\n",
    "The random_state parameter sets the random seed for reproducibility. By fixing the random seed, the algorithm produces consistent results across multiple runs, which is useful for debugging and comparing different parameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If a data point has only 2 neighbours of the same class within a radius of 0.5, its anomaly score using KNN with K=10 would be high. This is because KNN works by ranking neighboring data points according to their distance and assigning a label based on the majority of their neighbors. In this case, with only 2 neighbors of the same class, the data point is far from the majority, resulting in a high anomaly score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the number of neighbors within the radius\n",
    "neighbors_within_radius = 2\n",
    "\n",
    "# Calculate the missing neighbors required to reach k=10\n",
    "missing_neighbors = 10 - neighbors_within_radius\n",
    "\n",
    "# Compute anomaly score\n",
    "anomaly_score = missing_neighbors / 10  # Normalize by k\n",
    "\n",
    "print(\"Anomaly Score:\", anomaly_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9.** Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 0.7957282801307705\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the harmonic number H(i)\n",
    "\n",
    "def harmonic_number(i):\n",
    "    return np.sum(1 / np.arange(1, i + 1))\n",
    "\n",
    "# Function to calculate average path length in isolation forest\n",
    "def avg_path_length(n):\n",
    "    return 2 * harmonic_number(n - 1) - (2 * (n - 1) / n)\n",
    "\n",
    "# Given parameters\n",
    "num_trees = 100\n",
    "num_data_points = 3000\n",
    "avg_path_length_data_point = 5.0\n",
    "\n",
    "# Calculate average path length in isolation forest\n",
    "c_n = avg_path_length(num_data_points)\n",
    "\n",
    "# Calculate anomaly score\n",
    "anomaly_score = 2 ** (-avg_path_length_data_point / c_n)\n",
    "\n",
    "print(\"Anomaly Score:\", anomaly_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
